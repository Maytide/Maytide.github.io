---
layout: post
title:  "Quickdraw Gifs Draft"
date:   2017-07-25 15:00:00 -0400
author: George Zhang
categories: sketch tensorflow 
custom_css: quickdraw
---

<!--
{% if page.custom_css %}
  {% for stylesheet in page.custom_css %}
  <link rel="stylesheet" href="/css/{{ stylesheet }}.css" media="screen" type="text/css">
  {% endfor %}
{% endif %}
-->

<!--http://mattgemmell.com/page-specific-assets-with-jekyll/-->
<!--https://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll-->

<span align="center" class="quickdraw-samples">
    <img src="http://i.imgur.com/esT7CSe.gif">
    <img src="http://i.imgur.com/qmBzM3w.gif">
    <img src="http://i.imgur.com/RwSc7sx.gif">
</span>

A few months back, Google introduced their new project [Quickdraw][quickdraw], aimed at collecting the doodles of internet users worldwide in order to train a neural network to recognize them. These days, I only seem to get hard doodles and the NN is never able to recognize my poor drawings. Anyway, they've made their dataset publicly availible and others have already used the data to do things such as [analyze how different cultures draw geometric figures][quickdraw-circle], and [use deep learning to create original images][quickdraw-circle]. In particular, the latter is able to use vector images and interpolate between drawings; I couldn't do anything so advanced myself, but I thought it would be fun to see if a simple NN is able to draw doodles better than I can.

I started with [this][draw] implementation of [*draw*][draw-paper] on github, which is explained well in [the user's blog post][draw-post]. It works on the well-known MNIST dataset, which is so widely used in machine learning that some libraries like tensorflow (which is the ML library this implementation of *draw* uses) include an option to import it directly. Being unfamiliar with tensorflow, it took a while to figure out how to get it to work with other images, but it wasn't too bad once I discovered that everything is done using numpy arrays and [Google's quickdraw dataset is availible as numpy bitmaps][quickdraw-data].

{% highlight python %}
# Old code using MNIST
data_directory = os.path.join(FLAGS.data_dir, 'mnist')
if not os.path.exists(data_directory):
	os.makedirs(data_directory)
train_data = mnist.input_data.read_data_sets(data_directory, one_hot=True).train # binarized (0-1) mnist data

# New code using Quickdraw
# draw_obj is any one of the quickdraw dataset objects
data_directory = os.path.join(FLAGS.data_dir, draw_obj)
if not os.path.exists(data_directory):
    os.makedirs(data_directory)

draw_data = np.load('../quickdraw_data/%s.npy' % (draw_obj,))
draw_data = draw_data / 255 # Normalize the 8-bit values to somewhere between [0,1]
{% endhighlight %}

More conveniently, they were also the same size (28 x 28 px)! Because of conveniences like this, there really wasn't much I had to change.

The original implementation worked well with all ten digits in the dataset during training, being able to draw any one of them without turning into a jumbled mess influenced by the figure of other digits. Despite this, I saw no need to include multiple different objects in one training session (though it may be something worth trying in the future).

Alright, so in the end it worked, but there is a catch to it I'll talk about that later. First, let's see how the generated images look like; the images shown below were generated by the NN model, with the only modification being the application of a sharpening filter.

As with all ML tasks, the performance was heavily reliant on how much data I had to train the model. Most of the quickdraw datasets contained over 100k doodles, which might have worked for 1000 iterations on simple shapes like 'hexagon' or 'octagon' but definitely would not have worked for more complex drawings like 'whale' or 'airplane'. 

So to increase the number of training iterations, I wrote a simple generator that could iterate over the same dataset multiple times once I was nearing the end. For a classification task, this would not be a good solution due to overfitting, but for this simple project overfitting really isn't that big of an issue.

{% highlight python %}
# 100k+ training images with a batch size of 100 per training iteration
split_size = n_training_images // batch_size
def get_next_batch(n=batch_size):
    k = 0 # Keeps count of where we are iterating in draw_data
    # "Reset" generator to beginning of dataset if nearing end
    while k <= split_size - 1:
        if k >= split_size - 2:
            k = randint(0, n - 1)
        else:
            k += 1
        yield draw_data[k:k + n]
{% endhighlight %}

For the hexagons, I used 10,000 iterations of training with each batch containing 100 images:

<span align="center" class="quickdraw-samples">
    <img src="http://i.imgur.com/64tkVKJ.png">
    <img src="http://i.imgur.com/agGYk6l.png">
    <img src="http://i.imgur.com/5bwHe0Y.png">
    <img src="http://i.imgur.com/KbEo5Qb.png">
</span>

*One of these is not like the others...*

Not bad. For the more complicated figures, I trained for 25,000 - 40,000 iterations, keeping all other parameters invariant.

<span align="center" class="quickdraw-samples">
    <img src="http://i.imgur.com/rP8iDan.png">
    <img src="http://i.imgur.com/yIwL7Ly.png">
    <img src="http://i.imgur.com/ZtkQLj0.png">
    <img src="http://i.imgur.com/8n9lauH.png">
</span>

<span align="center" class="quickdraw-samples">
    <img src="http://i.imgur.com/t3CVUzD.png">
    <img src="http://i.imgur.com/PzE7CMq.png">
    <img src="http://i.imgur.com/pKXeR7A.png">
    <img src="http://i.imgur.com/oEYS7Jg.png">
</span>

<span align="center" class="quickdraw-samples">
    <img src="http://i.imgur.com/HVa0AmE.png">
    <img src="http://i.imgur.com/TmC7DgR.png">
    <img src="http://i.imgur.com/SvPxcBv.png">
    <img src="http://i.imgur.com/Pu3KXrX.png">
</span>

<span align="center" class="quickdraw-samples">
    <img src="http://i.imgur.com/NbFCNTm.png">
    <img src="http://i.imgur.com/sAGfXJl.png">
    <img src="http://i.imgur.com/puNGX5Z.png">
    <img src="http://i.imgur.com/vlVt8AZ.png">
</span>


The variation is much more apparent in these. At the very least, they all seem *plausible* as doodles of ... something. See if you can guess what the objects are before revealing the answers below.

<!--
<div class="hidden-hover">
    <div class="hidden-text">Coffee Cup</div>
</div>
<div class="hidden-hover">
    <div class="hidden-text">Whale</div>
</div>
<div class="hidden-hover">
    <div class="hidden-text"><s>Palm Tree</s> Windmill</div>
</div>

<div>Test</div>
<div class="hidden-hover">Test 2</div>
<div class="hidden-text">Test 3</div>
<div class="hidden-hover">
    <div>Test 4</div>
</div>

Test
-->

<!--https://stackoverflow.com/questions/32814161/how-to-make-spoiler-text-in-github-wiki-pages-->
<details> 
  <summary>Image Set 1</summary>
   Apple, 25k iterations
</details>
<details> 
  <summary>Image Set 2</summary>
   Coffee Cup, 25k iterations
</details>
<details> 
  <summary>Image Set 3</summary>
   Whale, 40k iterations
</details>
<details> 
  <summary>Image Set 4</summary>
   <s>Palm Tree</s> <s>Scimitars</s> <s>Biohazard</s> Windmill, 25k iterations
</details>

[Conclusion]

Now I want to talk about the catch to what I have been able to due using *draw*. In generating the images, one part of the code stands out as kind of suspicious:

{% highlight python %}
for xtrain in get_next_batch():
    # xtrain is a batch of 100 samples used to train the NN,
    # obtained from the generator in the previous code snippet.
    feed_dict = {x:xtrain}
    ...
...

canvases = sess.run(cs,feed_dict) # generate some examples
canvases = np.array(canvases) # T x batch x img_size
{% endhighlight %}

You'll notice that in generating the new samples, we used the feed_dict variable from the last iteration of training! So is it possible that we are somehow abusing the sample images to draw our new images? (Well, in some sense, of course we are ... this is machine learning, after all. To make the question more precise, is the neural net actually drawing new images based off of statistical data from training images, or "copying" off of the training images like it's a university rewording an essay they found on the internet?)

Well, I'm not entirely sure to what extent the feed_dict influences the generated images, but they seem reasonably original to me. To back it up, 

<!--*Coffee cups.*-->
<!--
*Take a guess!*
Could you figure out what they were? If you guessed fish, you're wrong, because they're supposed to be whales. Okay, okay, maybe it wasn't that hard to tell. But if you can guess what this next one is, I'd be surprised.
*Palm Tree, Scimitars or Biohazard?*
-->
<!--Let's take a look at something even more complicated, 40,000 iterations of...-->



[sketch-boat]: http://sketchtoy.com/68231806
[sketch-plane]: http://sketchtoy.com/68231810
[sketch-truck]: http://sketchtoy.com/68231812
[quickdraw]: https://quickdraw.withgoogle.com/
[quickdraw-circle]: https://qz.com/994486/the-way-you-draw-circles-says-a-lot-about-you/
[quickdraw-magenta]: https://qz.com/994486/the-way-you-draw-circles-says-a-lot-about-you/
[draw]: https://github.com/ericjang/draw
[draw-post]: http://blog.evjang.com/2016/06/understanding-and-implementing.html
[draw-paper]: https://arxiv.org/pdf/1502.04623.pdf
[quickdraw-data]: https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/numpy_bitmap